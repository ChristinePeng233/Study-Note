# 爬虫

## 什么是爬虫？

- 网络爬虫是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本
- 网页中有很多的信息和数据，要想从网页中抓取信息和数据并保存到本地，这就是网络爬虫
- 网页一般是由**HTML**语言编写的，如果说我们要从网页抓取内容，其实就是在**HTML**中，找到相应的内容进行抓取



## 认识网页和HTML

大部分的网页有三部分组成：

- **HTML**（超文本标记语言）
  	整个网页的结构，类似于人的骨架，它定义了你眼睛、嘴巴、鼻子的位置应该放在哪里。

- **CSS**（层叠样式表）

  ​	CSS表示样式，定义了外观，类似于人的外观细节，眼睛是双眼皮还是单眼皮，鼻子的大小等。

- **JScript**（活动脚本语言）

​		Jscript表示功能，网页中的交互内容，交互特效都包含其中，类似于人的技能，你会网络爬虫，数据挖掘，数据可视化等

## 如何查看网页的HTML编码

打开Google Chrome，鼠标右击，菜单栏内选择【检查】，就可以看到HTML源码了

![1695257145507](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695257145507.png)

我们观察到这里又好多的<>构成，这些我们称为**标签**。也就是说html是由标签组成的，那么这些标签有什么样的特征呢？

## 标签

![](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695257714824.png)

在baidu这样一个简单的网页中，我们看到标签主要有两种类型：

1. <head></head>、<div></div>

   像这些是<>和</>成对出现的标签，我们称为**闭合标签**

2. <input >
   像这些是< >**自闭和标签**

![1695258001479](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695258001479.png)

在这两种标签里都有一些标签的**属性**

例如：

​	id=“”，style=“”，class=“”

​	都表示着对应标签的属性

## “定位的方法”

- 点击方框内的图标，再移动你的鼠标到你想找的内容上时，Html中就定位到了相应的内容了。

![1695259021000](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695259021000.png)

- 鼠标移动到相应内容上，点击鼠标右键，选择【检查】，HTML中同样定位到了相关内容上。

![1695259117817](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695259117817.png)

## 熟练掌握HTML页面

![1695259266138](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695259266138.png)

| 常见标签            | 用法或意义               |
| ------------------- | ------------------------ |
| <ul>..</ul>         | 表示无序列表             |
| <html>..</html>     | 表示标记中间的元素是网页 |
| <body>..</body>     | 表示用户可见的内容       |
| <div>..</div>       | 表示框架                 |
| <p>..</p>           | 表示段落                 |
| <li>..</li>         | 表示列表                 |
| <img>..</img>       | 表示图片                 |
| <h1>..</h1>         | 表示标题                 |
| `<a href="">..</a>` | 表示超链接               |



## 网络响应

![1695262479177](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695262479177.png)

我们已知IP向网页服务器发送了IP，即访问请求，服务器返回html。这个过程我们称为**网络请求**，也称为HTTP请求。

常见的协议有以下八种，其中GET和POST最常用

| 请求方式 | 描述                                                         |
| -------- | ------------------------------------------------------------ |
| GET      | 发送请求来获得服务器上的资源【比如请求百度的页面】           |
| POST     | 向服务器提交资源让服务器处理【比如百度云盘上传资源】         |
| HEAD     | 主要用来检查资源或超链接的有效性或是否可达、检查网页是否被串改或更新 |
| PUT      | 向指定资源位置上上传其最新内容                               |
| DELETE   | 请求服务器删除其资源                                         |
| CONNECT  | HTTP/1.1协议中预留给即将连接改为管道方式的代理服务器         |
| OPTIONS  | 允许客户端查看服务器的性能                                   |
| TRACE    | 回显服务器收到的请求，主要用于测试或诊断                     |

(1)GET请求：一般情况下，发送请求从服务器上获取资源，不会对服务器资源产生任何影响的时候使用GET请求

(2)POST请求：向服务器发送数据（登录）、上传文件等让服务器处理，会对服务器资源产生影响的时候使用POST请求



## 请求头常见参数

(1)User-Agent

浏览器的名称。向服务器发送请求时，服务器通过该参数了解请求是从何种浏览器发出，以便返回正确的信息。必须设定这个参数。不设置该参数，服务器就知道你是用python等工具在爬取数据，此时，服务器设置了反爬，就会拒绝你的请求

![1695269491721](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695269491721.png)



(2)Referer

表明网页是从哪个网站跳转而来。

有些网页只能从特定的链接跳转过来。如果不设置，一些网页就会监测到你是通过非浏览器方式试图获取他们的html页面，就不会返回数据。通过该参数使用来进行反爬虫设计，我们需要设定这个参数。

(3)Cookie

HTTP协议是无状态的。我们在浏览器上发送了两次请求，服务器不知道这两个请求是否来自同一个人。因此这时候就用Cookie来做标识。一般登陆后才能访问的网页，这个时候就需要发送Cookie信息。

注：以上的信息，在爬虫时，复制粘贴就可以了，相当于我们用python“伪装”成一个浏览器，一个活生生的人去访问服务器，得到html，并且在html中获取数据，这就是爬虫正在做的事情



## 常见响应状态码

| 状态码 | 含义                                                         |
| ------ | ------------------------------------------------------------ |
| 200    | 请求成功                                                     |
| 301    | 永久重定向，资源（网页等）被永久转移到其它URL【www.jingdong.com --->>>www.jd.com】 |
| 404    | 请求的资源（网页等）不存在                                   |
| 500    | 内部服务器错误                                               |



## 认识URL

URL是Uniform Resource Locatior的缩写，中文全称统一资源定位符，俗称网址。每一个信息资源在网上都有唯一的一个地址，这就是URL。

我们输入www.baidu.com,浏览器其实还帮我们做了些事，比如我们输入的网址不满足URL的规则，浏览器会默默的帮我们补充上https://



## URL的组成

URL遵守以下的语法规则

scheme://host:port/path/?query-string=xxx#anchor

scheme - 定义因特网服务的类型，即访问因特网的协议。常见的协议有http、https、ftp、file

<1>HTTP协议：超文本传输协议，是一种发布和接收HTML页面的协议。默认服务端口如80端口

<2>HTTPS协议：是HTTP协议的加密版本，在HTTP下加入了SSL层。默认服务端口为443端口

<3>FTP协议：文件传输协议

<4>FILE协议：本地文件传输协议。

http为明文传输，不安全，容易被截取相关信息，如登录QQ时，别人窃取了你的登录信息。

https则时加密传输，相对安全。登录QQ时别人截取到的也是经过加密后的登录信息。

FTP，例如QQ在传输文件时，就遵循了FTP协议

FILE协议则时本地文件访问过程中遵循的协议。如下载下来的网页，再打开，它的URL发生了变化。

host - 主机名，域名（比如：www.baidu.com）

port - 定义主机上的端口号（http的默认端口号是80；https的默认端口号是443）

path - 定义服务器上的路径（如果省略，则文档必须位于网站的根目录中）

![1695275884239](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695275884239.png)

query-string - 定义文档/资源的名称

![1695276031399](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695276031399.png)

anchor-网页锚点，前端用来做网页定位

![1695276316063](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695276316063.png)



## 爬虫的流程

1. 得到HTML页面：即发出http请求
2. 按照规则进行数据的提取
3. 数据的存储：excel，txt，csv，sql等

### http请求

模拟http请求的过程是极为复杂的，用户需要向**DNS**发送一个**请求**以**获取IP**，再将**获取的IP**发送给**服务器**，**服务器**返回html。

这个过程可以由python代为完成，但是直接用python做也是麻烦的。

但幸好我们有相应的库来帮助我们快速的完成这件事

一般来说，python有两个基础的库来完成模拟请求。

1. urllib
   - python自带，不需要额外安装
   - 使用起来有点麻烦
2. requests
   - 不是内置库，需要额外安装
   - 使用相对方便

我们使用requests库

#### Resquests库

- `response.text`
  返回的结果，不仅没有找到找到我们想要的数据，而且出现了一串奇怪的字符！
  原因：编码有误！
  `response.text`返回了requests猜测的编码方式，不幸它猜错了
  大部分中文网站，utf8
  解析网页，网页写的时候用了一个编码，解析的时候也需要相同的编码

  ![1695281967164](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695281967164.png)

- `response.content`
  返回的bytes流数据，我们获取这样的数据，再自己完成编码的转换工作
  `print(response.content.decode('utf8'))`指定用utf8编码解析

补充：

- `response.encoding`
  `response.txt`查看text方法猜测了何种编码
  ![1695283277111](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695283277111.png)
- `response.status_code`
  requests提供了访问url的状态响应码
- 字典
  ![1695284007678](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695284007678.png)

​	按照我们的设想，对其get请求并解码

​	部分版本中，上述代码会报错，在不报错的情况下，也不建议大家用这样的方式写URL

​	原因是**在网络中的数据是由bytes传输的，番茄属于中文，并不是bytes数据**

​	我们借助字典帮助我们完成URL的建立

![1695288653542](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695288653542.png)

返回的HTML太短，不像是一个正确的结果

![1695288867798](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695288867798.png)

问题在哪？

python模仿人去发送请求装的不够像！

#### 设置请求头参数

设置一个名为headers的字典作为参数

在网页中【检查】→【network】→【Request Headers】

![1695290025478](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695290025478.png)



![1695292953899](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695292953899.png)

#### 登录信息

找到【Preserve log】并勾选，它能帮助我们留下登录信息，使得登录后的页面信息部替换掉之前的信息

完成上述工作后再来填写登录信息，并点击登录

![1695299841604](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695299841604.png)

【playload】所有的输入的data信息都会出现在这里，可以提取这里的data信息，使你的爬虫伪装成了真实的人在登录并获取html信息

**注：**这里最重要的就是name和password，有的密码是以明文形式呈现，有的设置安全措施，不显示密码

![1695299927343](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695299927343.png)



![1695300240515](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695300240515.png)

1.豆瓣在登录的url的链接里做了手脚，以防止爬虫，我们需先登录豆瓣，找到**reference**中的网址

或者在检查中找到对应代码中的网址

![1695301295313](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695301295313.png)

2.请求头的伪装务必完成

3.输入一个名为data的字典，将【From Data】中的内容以字典的形式输入，越完整越好（伪装的越像）

`requests.session()`:它可以自动处理cookies，做状态保持

![1695299748506](C:\Users\86133\Documents\WeChat Files\wxid_qc40skhcsj4722\FileStorage\Temp\1695299748506.png)



## 正则表达式

### 一.单字符匹配

#### 1.匹配某个字符串：`match()`:只能够匹配某个

match(表达式，匹配对象)

tips1：只能匹配某个，并且返回的结果是object

tips2：`match()`匹配不到，不会报错，返回None

=>使用`group()`的方法进行打印

tips3：`match()`是从起始位置进行匹配

```
import re
text = 'python'
result = re.match('py',text)
print(result.group())
```

![1](E:\study information\myself\爬虫\正则表达式\1.png)

#### 2.点(.)：匹配任意的某个字符

```
import re
text = 'python'
result = re.match('.',text)
print(result.group())
```

![](E:\study information\myself\爬虫\正则表达式\2.png)

**注：**

**(1)无法匹配换行符**

**(2)从起始位置进行匹配**



#### 3.\d：匹配任意的某个数字 `[0-9]`

```
import re
text = '2python'
result = re.match('\d',text)
print(result.group())
```

![](E:\study information\myself\爬虫\正则表达式\3.png)

**注：**

**1.只能匹配数字，除了数字以外的内容均不匹配**

**2.从起始位置进行匹配**



#### 4.\D:除数字外均可匹配`[^0-9]`

\d和\D可以理解为互补的关系

**注：**

**1.除了数字外的内容均可匹配**

**2.从起始位置进行匹配**

```
import re
text = '_python'
result = re.match('\D',text)
print(result.group())
```

![](E:\study information\myself\爬虫\正则表达式\4.png)

#### 5.\s：匹配空白字符

**注：**

**1.\s,这里的s为小写**

**2.匹配空白字符**

**3.从起始位置进行匹配**

**4.\n、\t、\r、空格都表示空白字符**



#### 6.\w：匹配小写的a-z，大写的A-Z，数字和下划线`[0-9a-zA-Z_]`

**注：**

**1.\w,这里的w为小写**

**2.从起始位置进行匹配**

**3.\n、\t、\r、空格都表示空白字符**

**4.除去上述的都无法匹配（如中文、中文符号）**



#### 7.\W：匹配除小写\w之外的所有字符`[^0-9a-zA-Z_]`

**注：\w和\W可以理解为互补的关系**

#### 8.[]：->>组合的方式，只要在括号内的内容[\d\D]

注：1.[]内的内容都可以匹配

​	2.[]内多个匹配内容时，取“或”，只要匹配对象中含有其中一个内容就匹配

​	3.从起始位置进行匹配

​	4.（本节）仅匹配某个字符

```
import re
text = '?python'
result = re.match('[?]',text)
print(result.group())
```

![](E:\study information\myself\爬虫\正则表达式\5.png)

### 二.多字符匹配

#### 1.星号（*）：匹配0个或者多个字符

```
import re
text = '15278444547'
result = re.match('[\d]*',text)
print(result.group())
```

![](E:\study information\myself\爬虫\正则表达式\6.png)

```
import re
text = '152.784445.47'
result = re.match('[\d]*',text)
print(result.group())
```

![](E:\study information\myself\爬虫\正则表达式\7.png)

#### 2.加号（+）：匹配一个或者多个

从起始位置开始匹配

```
import re
text = '152.784445.47'
result = re.match('[\d]+',text)
print(result.group())
```

![](E:\study information\myself\爬虫\正则表达式\7.png)

#### 3.问号(?):要么匹配0个，要么匹配1个

从起始位置开始匹配。匹配0次报空，匹配1次，报匹配内容

#### 4.{m}：匹配指定个数m

从起始位置匹配数字4次，第4次是.，不满足匹配要求，报错

```
import re
text = '152.784445.47'
result = re.match('[\d]{4}',text)
print(result.group())
```

![](E:\study information\myself\爬虫\正则表达式\9.png)

#### 5.{m,n}：匹配m到n个

```
import re
text = '152.784445.47'
result = re.match('[\d]{1,7}',text)
print(result.group())
```

![](E:\study information\myself\爬虫\正则表达式\7.png)

默认匹配最多个，这里指匹配1到7次

### 三.特殊匹配

1.点(.):匹配任意的某个字符

[.]表示的是仅表示匹配点(.)，即真实的点[.]

.+表示取从起始开始的任意字符

```
import re
text = '152.784445.47///'
result = re.match('.+',text)
print(result.group())
```

![](E:\study information\myself\爬虫\正则表达式\10.png)

### 四.正则表达式的小案例

#### 1.验证手机号

手机号的特点[11位数字]

第1位：1

第2位：[3456789]

第3位至第11位：[0-9]

```
import re
text = '13367910005'
result = re.match('1[3456789][0-9]{9}',text)
print(result.group())
```

#### 2.验证邮箱

邮箱的特点：

用户名部分（@前的部分）：英文字母、数字、下划线组成

域名部分（@后的部分）：数字，字母（一般都是小写）

```
import re
text = '1024145070@qq.com'
result = re.match('\w+@[0-9a-z]+[.]com',text)
print(result.group())
```

#### 3.验证身份证号[18位]

身份证号特点：

前17位：[0-9]

18位：[0-9xX]

```
import re
text = '36010220040322082x'
result = re.match('[0-9]{17}[0-9xX]',text)
print(result.group())
```

#### 4.实战-百度图片的抓取

```
import requests
#设置请求头
headers = {
    "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36"

}
url = "https://img.cc0.cn/pixabay/2019102118444574.jpg%21cc0.cn.jpg"
#发送get请求
response = requests.get(url,headers=headers)
#变为byte流数据
content = response.content
#保存图片
with open('tomato.jpg','wb') as f:
    f.write(content)
```

其中，w：write，b：bytes

- 如果图片空白
  1. 本身图的url出现问题
     如果url（图片格式）显示的jpg，那就保存为jpg
     如果url（图片格式）显示的jpeg，那就保存为jpeg
     ...
  2. 格式保存不匹配，导致错误

```python
import requests
import re
pic = input("请输入你想爬取的数据？\n")
#设置请求头
headers = {
    "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36"

}
url = "https://image.baidu.com/search/index?tn=baiduimage&ps=1&ct=201326592&lm=-1&cl=2&nc=1&ie=utf-8&dyTabStr=MTEsMCwxLDYsMyw0LDUsMiw4LDcsOQ%3D%3D&word="
#设置关键词
kw = {
    "word":pic
}
#发送get请求
response = requests.get(url,headers=headers,params=kw)
#对搜索页面是需要解析的
content = response.content.decode('utf8')
#数据提取
detail_urls = re.findall('"objURL":"(.*?)"',content,re.DOTALL)
#图片下载'
i = 0
for detail_url in detail_urls:
    response = requests.get(detail_url,headers=headers)
    content = response.content
    if detail_url[-3:] == 'jpg':
        with open('{}.jpg'.format(i), 'wb') as f:
            f.write(content)
    elif detail_url[-4:] == 'jpeg':
        with open('{}.jpeg'.format(i),'wb') as f:
            f.write(content)
    elif detail_url[-3:] == 'png':
        with open('{}.png'.format(i),'wb') as f:
            f.write(content)
    elif detail_url[-3:] == 'bmp':
        with open('{}.bmp'.format(i),'wb') as f:
            f.write(content)
    else:
        continue
    i += 1
```

- 为什么百度图片抓取时，只得到30个页面？
  - 所有的图片都存放在ul标签下的li标签内，此时滚动鼠标滑轮，大家会注意到html页面上会多出来一些div标签，图片变多了，这就是ajex技术。
    ajex技术可以通过js实现一些交互，从而通过滚轮华东得到下一个div标签（即下一页）获取更多的图片
    我们现有的知识没办法突破ajex技术，我们换一种解决方法
  - ![](E:\study information\myself\爬虫\正则表达式\11.png)
  - ![](E:\study information\myself\爬虫\正则表达式\12.png)